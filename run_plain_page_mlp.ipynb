{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "import plain_page\n",
    "import load_data\n",
    "import neur_nets\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# want cuda? set this to True\n",
    "cuda = False\n",
    "\n",
    "# load needed dataset, normalized with l2 for compliance with ProxSARAH's authors methodology\n",
    "# this gives tensors already\n",
    "train_x, train_y = load_data.load_mnist_train_l2()\n",
    "test_x, test_y = load_data.load_mnist_test_l2()\n",
    "\n",
    "# flatten for compliance with ProxSARAH's authors methodology\n",
    "train_x = torch.flatten(train_x, start_dim=1)\n",
    "test_x = torch.flatten(test_x, start_dim=1)\n",
    "\n",
    "if cuda:\n",
    "    train_x, train_y = train_x.cuda(), train_y.cuda()\n",
    "    test_x, test_y = test_x.cuda(), test_y.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_hidden_layers = 100 # in compliace with the network used by ProxSARAH's authors\n",
    "num_out_classes = 10\n",
    "\n",
    "# use needed loss function\n",
    "lossfunc = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0, 0.31738948822021484, 60000, 2.30395, 0.09730\n",
      "215, 1.7267932891845703, 172216, 2.28129, 0.09820\n",
      "461, 3.0138027667999268, 232240, 2.20435, 0.23790\n"
     ]
    }
   ],
   "source": [
    "# instantiate needed NN\n",
    "model = neur_nets.MLP(train_x.size(dim = 1), num_hidden_layers, num_out_classes, cuda)\n",
    "\n",
    "b = len(train_y) # batch size, =n in this case\n",
    "\n",
    "eta = 0.045 # we have \"manually binary searched\" for the best hyper-param\n",
    "T = 500 # number of iterations\n",
    "\n",
    "print_log = True\n",
    "print_msgs = False\n",
    "file_name_log = None # set this to file name to generate log file\n",
    "save_log = file_name_log != True\n",
    "# prints iteration number, elapsed time, computed gradients, train loss, test accuracy\n",
    "# the same is logged to file if file_name_log is not None\n",
    "_ = plain_page.train_with_plain_page(train_x, train_y, test_x, test_y, model, lossfunc, T, b, eta, file_name_log, save_log, print_log, print_msgs, as_sgd = False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8ce64ca77d62604bb7077990a444cb5c66dd6b48d20e4430a2b0cf9fefd028bc"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('envmlproj1': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

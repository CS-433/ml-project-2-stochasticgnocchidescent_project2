{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "import plain_page\n",
    "import load_data\n",
    "import neur_nets\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# want cuda? set this to True\n",
    "cuda = False\n",
    "\n",
    "# load needed dataset, normalized with l2 for compliance with ProxSARAH's authors methodology\n",
    "# this gives tensors already\n",
    "train_x, train_y = load_data.load_mnist_train_l2()\n",
    "test_x, test_y = load_data.load_mnist_test_l2()\n",
    "\n",
    "# flatten for compliance with ProxSARAH's authors methodology\n",
    "train_x = torch.flatten(train_x, start_dim=1)\n",
    "test_x = torch.flatten(test_x, start_dim=1)\n",
    "\n",
    "if cuda:\n",
    "    train_x, train_y = train_x.cuda(), train_y.cuda()\n",
    "    test_x, test_y = test_x.cuda(), test_y.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_hidden_layers = 100 # in compliace with the network used by ProxSARAH's authors\n",
    "num_out_classes = 10\n",
    "\n",
    "# use needed loss function\n",
    "lossfunc = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0,0.3550262451171875,60000,2.30289,0.11350\n",
      "78,1.0640828609466553,138788,2.29347,0.15020\n",
      "196,1.987273931503296,227336,2.29314,0.17290\n",
      "294,2.9333152770996094,311004,2.28793,0.20020\n",
      "540,4.142132043838501,371028,2.27592,0.34490\n",
      "771,5.643342733383179,487148,2.10800,0.36900\n"
     ]
    }
   ],
   "source": [
    "# instantiate needed NN\n",
    "model = neur_nets.MLP(train_x.size(dim = 1), num_hidden_layers, num_out_classes, cuda)\n",
    "\n",
    "b = len(train_y) # batch size, =n in this case\n",
    "\n",
    "eta = 0.045 # we have \"manually binary searched\" for the best hyper-param\n",
    "T = 1000 # number of iterations\n",
    "\n",
    "print_log = True\n",
    "print_msgs = False\n",
    "file_name_log = None # set this to file name to generate log file\n",
    "save_log = file_name_log != None\n",
    "# we run the plainest version of PAGE, incorporating random return\n",
    "\n",
    "# prints iteration number, elapsed time, computed gradients, train loss, test accuracy\n",
    "# the same is logged to file if file_name_log is not None\n",
    "_ = plain_page.train_with_plain_page_random_return(train_x, train_y, test_x, test_y, model, lossfunc, T, b, eta, cuda, file_name_log, save_log, print_log, print_msgs, as_sgd = False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8ce64ca77d62604bb7077990a444cb5c66dd6b48d20e4430a2b0cf9fefd028bc"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('envmlproj1': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

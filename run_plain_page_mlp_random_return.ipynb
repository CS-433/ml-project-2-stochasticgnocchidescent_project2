{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "import plain_page\n",
    "import load_data\n",
    "import neur_nets\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load needed dataset, normalized with l2 for compliance with ProxSARAH's authors methodology\n",
    "# this gives tensors already\n",
    "train_x, train_y = load_data.load_mnist_train_l2()\n",
    "test_x, test_y = load_data.load_mnist_test_l2()\n",
    "\n",
    "# flatten for compliance with ProxSARAH's authors methodology\n",
    "train_x = torch.flatten(train_x, start_dim=1)\n",
    "test_x = torch.flatten(test_x, start_dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# want cuda? set this to True\n",
    "cuda = False\n",
    "\n",
    "num_hidden_layers = 100 # in compliace with the network used by ProxSARAH's authors\n",
    "num_out_classes = 10\n",
    "\n",
    "# use needed loss function\n",
    "lossfunc = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0,0.3129913806915283,60000,2.30466,0.13740\n",
      "246,1.621016263961792,120024,2.30406,0.13900\n",
      "281,2.1825642585754395,188320,2.28624,0.24450\n",
      "405,3.1116206645965576,278332,2.23776,0.14960\n",
      "651,4.308775186538696,338356,2.28983,0.22670\n",
      "711,4.950826168060303,412752,2.24866,0.15550\n",
      "771,5.67824649810791,487148,2.13475,0.24490\n",
      "839,6.3966264724731445,563496,2.23378,0.14910\n"
     ]
    }
   ],
   "source": [
    "# instantiate needed NN\n",
    "model = neur_nets.MLP(train_x.size(dim = 1), num_hidden_layers, num_out_classes)\n",
    "\n",
    "b = len(train_y) # batch size, =n in this case\n",
    "\n",
    "eta = 0.045 # we have \"manually binary searched\" for the best hyper-param\n",
    "T = 1000 # number of iterations\n",
    "\n",
    "print_log = True\n",
    "print_msgs = False\n",
    "file_name_log = None # set this to file name to generate log file\n",
    "\n",
    "# we run the plainest version of PAGE, incorporating random return\n",
    "\n",
    "# prints iteration number, elapsed time, computed gradients, train loss, test accuracy\n",
    "# the same is logged to file if file_name_log is not None\n",
    "_ = plain_page.train_with_plain_page_random_return(train_x, train_y, test_x, test_y, model, lossfunc, T, b, eta, file_name_log, True, print_log, print_msgs, as_sgd = False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8ce64ca77d62604bb7077990a444cb5c66dd6b48d20e4430a2b0cf9fefd028bc"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('envmlproj1': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
